{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bringing together all the samples for this run\n",
    "+ MutID refers to single mutation in any samples\n",
    "+ SampleMutID refers to a mutation in a specific sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = \"/Users/martinscience\"\n",
    "home = \"/Users/mahtin\"\n",
    "folder = os.path.join(home, \"Dropbox/Icke/Work/somVar/NHL-DLBCL/NextNEBNext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get AG Krönke info\n",
    "+ Patient - time point information is combined in SampleID\n",
    "+ these samples have to be allocated to the full subpopulation PCRs without overlaps\n",
    "+ load PCR setup --> load_kroenke_PCR()\n",
    "    * sample :: Primer=MutID\n",
    "+ load the primer coords and mutation coords for each MutID --> load_kroenke_primer\n",
    "    * MutID --> MutStartEnd + AmpliconRange\n",
    "    * we need primer coords for assessing overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:28:23.521102Z",
     "start_time": "2021-01-13T12:28:23.448858Z"
    }
   },
   "outputs": [],
   "source": [
    "def num2well(num):\n",
    "    '''\n",
    "    loads PCR setup for association sample :: MutID(PrimerName) -> SampleMutID\n",
    "    '''\n",
    "    \n",
    "    string = \"ABCDEFGH\"\n",
    "    char = string[(num - 1) % 8]\n",
    "    count = int((num - 1) / 8) + 1\n",
    "    return f\"{char}{count}\"\n",
    "\n",
    "\n",
    "def load_kroenke_SU(coop_file):\n",
    "    '''\n",
    "    loads SampleID :: UPN\n",
    "    '''\n",
    "    return pd.read_excel(coop_file, sheet_name=\"Pat_Proben\", engine=\"openpyxl\").loc[:, ['SampleID', 'UPN']].query('SampleID != \"na\"')\n",
    "\n",
    "def load_kroenke_PCR(coop_file):\n",
    "    '''\n",
    "    takes care of everything\n",
    "    '''\n",
    "    # load into df the \n",
    "    \n",
    "    coop_df = pd.read_excel(coop_file, sheet_name=\"PCR\", engine=\"openpyxl\")\n",
    "    # edit the columns\n",
    "    # list(coop_df.columns)\n",
    "    # Primername does not directly translate into MutID (is unique only for each patient!)\n",
    "    # strip() is important because of trailing whitespace!!!\n",
    "    coop_df['MutID'] = coop_df['PrimerName'].str.strip()\n",
    "    # SampleMutID is is fully unique without the UPN\n",
    "    coop_df['SampleMutID'] = coop_df['SampleID'] + \":\" + coop_df['MutID']\n",
    "    \n",
    "    # however, Primer+UPN is unique --> MutID\n",
    "    # first, get SampleID::UPN association\n",
    "    SU_df = load_kroenke_SU(coop_file)\n",
    "    coop_df = coop_df.merge(SU_df)\n",
    "    coop_df['MutID'] = coop_df['MutID'] + \"-UPN\" + coop_df['UPN'].astype(str)\n",
    "    \n",
    "    # compute the well coordinates from the PCR-Belegung\n",
    "    coop_df['Well'] = coop_df['PCR-Belegung'].apply(num2well)\n",
    "    # make Well categorical for easy sorting\n",
    "    coop_df['Well'] = pd.Categorical(coop_df['Well'], [a + str(b +1) for b in range(8) for a in \"ABCDEFGH\"])\n",
    "    # insert the info about repeated PCR\n",
    "    coop_df['status'] = np.char.multiply(\"PCRrepeat\", (coop_df['PCR-fail'] == coop_df['PCR-fail']).astype(int))\n",
    "    # reduce to important columns\n",
    "    coop_df = coop_df.loc[:, [\n",
    "        'SampleMutID',\n",
    "        'MutID',\n",
    "        'Well',\n",
    "        'SampleID',\n",
    "        'status'\n",
    "    ]]\n",
    "    \n",
    "    return coop_df\n",
    "\n",
    "def load_kroenke_primer(coop_file):\n",
    "    '''\n",
    "    load the primer coords and the MutID-specific Mutations\n",
    "    MutID --> MutStartEnd + AmpliconRange\n",
    "    '''\n",
    "    \n",
    "    # load the file\n",
    "    coop_primer_df = pd.read_excel(coop_file, sheet_name=\"Mutationen_hg38\", engine=\"openpyxl\")\n",
    "    \n",
    "    \n",
    "    # chr_pat = r\"(chr[0-9XY]+):([0-9]+)-([0-9]+)\"\n",
    "\n",
    "    # get the MutID (PrimerName + UPN) from Primer_Name_F + UPN\n",
    "    coop_primer_df['MutID'] = coop_primer_df['Primer_Name_F '].str.replace(r\"_F( and.*$)?\", \"\") + \"-UPN\" + coop_primer_df['UPN'].astype(str)\n",
    "    \n",
    "    # make Chr categorical for better sorting\n",
    "    coop_primer_df['Chr'] = pd.Categorical(coop_primer_df['Chr'], [f\"chr{i}\" for i in range(23)] + ['chrX', 'chrY'])\n",
    "    coop_primer_df = coop_primer_df.loc[:, [\n",
    "         'MutID',\n",
    "         'Gene',\n",
    "         'Chr',\n",
    "         'Mut Start ',\n",
    "         'Mut End ',\n",
    "         'fwdPrimer',\n",
    "         'revPrimer',\n",
    "         'Temp',\n",
    "         'AmpliconRange',\n",
    "    ]].rename({'Mut Start ': \"Start\", 'Mut End ': \"End\"}, axis=1)\n",
    "    return coop_primer_df\n",
    "\n",
    "def extract_AmpRange(df):\n",
    "    df[[\"AmpStart\", \"AmpEnd\"]] = df['AmpliconRange'].str.extract(r\"chr[0-9XY]+:([0-9]+)-([0-9]+)\")\n",
    "    for col in [\"AmpStart\", \"AmpEnd\"]:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df.drop(\"AmpliconRange\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_file = os.path.join(folder, \"Final_list_CVK.xlsx\")\n",
    "PCR_df = load_kroenke_PCR(coop_file)\n",
    "primer_df = load_kroenke_primer(coop_file)\n",
    "coop_df = PCR_df.merge(primer_df, on=\"MutID\").sort_values(\"Well\")\n",
    "coop_df = extract_AmpRange(coop_df)\n",
    "coop_df['Project'] = \"AG_Kroenke\"\n",
    "coop_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING!!!\n",
    "+ coop_df contains identicat SampleMutIDs with different mutations\n",
    "+ that is G5, B6, E6 and H6!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_df.sort_values('SampleMutID').query('Gene == \"TP53\"').loc[coop_df['MutID'].str.startswith(\"TP53_e7_1\"),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get our mutation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T17:17:49.735379Z",
     "start_time": "2021-01-13T17:17:49.579445Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pop_df(pop_file):\n",
    "\n",
    "    # cycle through the sheets and append\n",
    "    pop_dfs = []\n",
    "    for sheet in [f\"NHL{i}\" for i in [1,2,8]]:\n",
    "        pop_df = pd.read_excel(pop_file, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        # reduce to the relevant columns plus MutID\n",
    "        pop_df = pop_df.loc[:, [col for col in pop_df.columns if col != \"LEGENDE\" and not \"Unnamed\" in col]]\n",
    "        # add the G for all working samples in Vanessas columns\n",
    "        pop_df = pop_df.fillna(\"G\")\n",
    "        pop_df['SampleID'] = sheet\n",
    "        pop_df['MutID'] = pop_df['MutID'].str.replace(\"_\", \"-\")\n",
    "        for c in pop_df.columns:\n",
    "            if c != \"Primer\":\n",
    "                pop_df[c] = pop_df[c].str.replace(r\" .*\", \"\")\n",
    "        pop_dfs.append(pop_df)\n",
    "    pop_df = pd.concat(pop_dfs).rename(dict(Primer=\"Well\"), axis=1)\n",
    "    \n",
    "    # compute the well coordinates\n",
    "    pop_df['Well'] = pop_df['Well'].apply(num2well)\n",
    "    # make Well categorical for easy sorting\n",
    "    pop_df['Well'] = pd.Categorical(pop_df['Well'], [a + str(b +1) for b in range(8) for a in \"ABCDEFGH\"])\n",
    "    return pop_df\n",
    "\n",
    "def get_mut_df(mut_file):\n",
    "    # get the output columns that should be the same for our lists\n",
    "    select_cols = ['MutID',\n",
    "     'SampleID',\n",
    "     'Gene',\n",
    "     'Chr',\n",
    "     'Start',\n",
    "     'End',\n",
    "     'fwdPrimer',\n",
    "     'revPrimer',\n",
    "     'Temp',\n",
    "     'AmpliconRange']\n",
    "\n",
    "    # cycle through the sheets and append\n",
    "    mut_dfs = []\n",
    "    for sheet in [f\"NHL{i}\" for i in [1,2,8]]:\n",
    "        mut_df = pd.read_excel(mut_file, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        # get SampleID from sheet name\n",
    "        mut_df['SampleID'] = sheet\n",
    "        # get MutID from Chr and Start\n",
    "        mut_df['MutID'] = mut_df['Chr'] + \"-\" + mut_df['Start'].astype(int).astype(str)\n",
    "        mut_df['MutID'] = mut_df['Chr'] + \"-\" + mut_df['Start'].astype(int).astype(str)\n",
    "        # rename columns for harmonizing with COOP\n",
    "        mut_df = mut_df.rename({\"Note\": \"Temp\", 'rev_Primer': 'revPrimer', 'fwd-Primer':'fwdPrimer'}, axis=1).loc[:, select_cols]\n",
    "        mut_dfs.append(mut_df)\n",
    "    mut_df = pd.concat(mut_dfs)\n",
    "    return mut_df\n",
    "\n",
    "\n",
    "def make_indices(df):\n",
    "    '''\n",
    "    assigns indices from SampleID and returns the df with indices\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    index_df = df2.groupby(\"SampleID\").count().loc[:,\"SampleMutID\"].reset_index().reset_index().drop(\"SampleMutID\", axis=1)\n",
    "    return df2.merge(index_df, on=\"SampleID\")\n",
    "\n",
    "\n",
    "def merge_df(pop_df, mut_df):\n",
    "    '''\n",
    "    reshape the pop_df and add info fields\n",
    "    '''\n",
    "    \n",
    "    select_cols = ['MutID',\n",
    "     'SampleID',\n",
    "     'Gene',\n",
    "     'Chr',\n",
    "     'Start',\n",
    "     'End',\n",
    "     'fwdPrimer',\n",
    "     'revPrimer',\n",
    "     'Temp',\n",
    "     'AmpliconRange']\n",
    "    \n",
    "    \n",
    "    nhl_df = pop_df.merge(mut_df, on=[\"MutID\", \"SampleID\"]).set_index(select_cols + ['Well']).stack().reset_index().rename({0:\"status\", \"level_11\":\"type\"}, axis=1)\n",
    "    nhl_df['Project'] = \"NHL\"\n",
    "    nhl_df['SampleID'] = nhl_df['SampleID'] + \"-\" + nhl_df['type']\n",
    "    nhl_df['SampleMutID'] = nhl_df['SampleID'] + \":\" + nhl_df['MutID']\n",
    "    # make Chr categorical\n",
    "    nhl_df['Chr'] = pd.Categorical(nhl_df['Chr'], [f\"chr{i}\" for i in range(23)] + ['chrX', 'chrY'])\n",
    "    # extract Amplicon Range\n",
    "    nhl_df = extract_AmpRange(nhl_df)\n",
    "    # make primary index by counting the SampleIDs\n",
    "    nhl_df = make_indices(nhl_df)\n",
    "    return nhl_df.sort_values(['SampleID', 'Chr', 'Start'], ascending=True)\n",
    "\n",
    "\n",
    "def get_index_df(org_df):\n",
    "    '''\n",
    "    returns the sum of sequenczing samples per index\n",
    "    '''\n",
    "    \n",
    "    # assign microliters\n",
    "    df = org_df.copy()\n",
    "    df['µl'] = 2\n",
    "    df.loc[df['status'].str.startswith(\"F\"), 'µl'] = 0\n",
    "    df.loc[df['status'].str.startswith(\"P\"), 'µl'] = 4\n",
    "    index_df = df.loc[~df['status'].str.startswith(\"F\")].groupby(\"index\").agg({\"MutID\":\"count\", \"Project\":\"first\", \"µl\":\"sum\", \"SampleID\":\"first\"}).rename(dict(MutID=\"sampleCount\"), axis=1).reset_index()\n",
    "    \n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_file = os.path.join(folder, \"Subpop_PCR2.xlsx\")\n",
    "pop_df = get_pop_df(pop_file)\n",
    "### merge the Primers with respective mutations\n",
    "mut_file = os.path.join(folder, \"Subpop_mutations.xlsx\")\n",
    "mut_df = get_mut_df(mut_file)\n",
    "nhl_df = merge_df(pop_df, mut_df).sort_values(['SampleID', 'Chr', 'Start'], ascending=True)\n",
    "nhl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging coop into nhl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showOverlap(df, padding=10):\n",
    "    '''\n",
    "    finds overlaps between mutations\n",
    "    overlaps are defined as mutations between different samples at overlapping positions in the same index\n",
    "    \n",
    "    requires that identical SampleMutID (same sample, same Primers for different mutation) has been deduplicated\n",
    "    --> has to be improved by gap group assignment (LEGACY code)!\n",
    "    '''\n",
    "    \n",
    "    # checks if the df has any overlaps as described above\n",
    "    full_df = df.copy()\n",
    "    # df has to be sorted by index, amplicon start site and then by SampleID\n",
    "    full_df = full_df.sort_values(['index', \"Chr\", \"AmpStart\", \"SampleID\"])\n",
    "    # make an integer for any kind of overlap\n",
    "    full_df['internal_overlap'] = ((full_df['AmpStart'] + padding < full_df.shift(1)['AmpEnd']) & (full_df['Chr'] == full_df.shift(1)['Chr']) & (full_df['index'] == full_df.shift(1)['index']) & (full_df['SampleID'] != full_df.shift(1)['SampleID'])).astype(int)\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def hasOverlap(df, padding=10):\n",
    "    # boolean checks if the df has any overlaps as described above\n",
    "    full_df = showOverlap(df)\n",
    "    return len(full_df.query(\"internal_overlap == 1\")) > 0\n",
    "\n",
    "\n",
    "\n",
    "# get the index with the fewest sample for equal allocation\n",
    "def get_min_index(df, black_list=[], offset=0):\n",
    "    '''\n",
    "    get the index containing fewest samples from samples exluding black_list\n",
    "    '''\n",
    "    # remove -1 index\n",
    "    index_df = get_index_df(df.query(\"index != -1\")).query(\"SampleID not in @black_list\").sort_values(\"sampleCount\", ascending=True).iloc[offset]['index']\n",
    "    \n",
    "    return index_df\n",
    "\n",
    "# black_list = [f\"NHL{i}-{t}\" for i in [1,2,8] for t in [\"WB\", \"GR\"]]\n",
    "# index_df = get_index_df(nhl_df)\n",
    "# index_df.sort_values(\"sampleCount\")[:10]\n",
    "# get_min_index(nhl_df, offset=1, black_list=black_list)\n",
    "\n",
    "\n",
    "def merge_indices(main_df, df, black_list=[]):\n",
    "    '''\n",
    "    takes an indexed main_df and tries to merge df into it\n",
    "    1) non-overlapping rows in df are bundled into one index\n",
    "    2) internally overlapping mutations are assigned to index in main_df with fewest samples\n",
    "    '''\n",
    "    # remove any duplicate SampleMutID\n",
    "    # they do exist!! need to be seen by pipeline but not by indexing\n",
    "    df = df.drop_duplicates(\"SampleMutID\")\n",
    "\n",
    "    # set arbitrary index for checking overlaps\n",
    "    df['index'] = -1\n",
    "    # assign internal overlaps\n",
    "    df = showOverlap(df)\n",
    "    \n",
    "    #### ASSIGN NON-OVERLAPPING MUTATIONS TO NEW INDEX ########\n",
    "    \n",
    "    \n",
    "    # make new index for non_overlapping samples\n",
    "    next_index = get_index_df(main_df)['index'].max() + 1\n",
    "    print(\"Assigning non-overlapping bulk to index = \", next_index)\n",
    "    df.loc[df[\"internal_overlap\"] == 0, \"index\"] = next_index\n",
    "    \n",
    "    #### ASSIGN OVERLAPPING MUTATIONS ########\n",
    "    # add overlap_df to main_df and \n",
    "    all_df = pd.concat([main_df, df.query('internal_overlap == 1')]).reset_index(drop=True).drop('internal_overlap', axis=1)\n",
    "\n",
    "    overlap_df = all_df.query('index == -1')\n",
    "    print(f\"Assigning overlapping {len(overlap_df.index)} samples to existing index groups\")\n",
    "    \n",
    "    # step through rows\n",
    "    for i, row in overlap_df.iterrows():\n",
    "        # get the current index\n",
    "        index = row.name\n",
    "        new_index = get_min_index(all_df, black_list=black_list)\n",
    "        print(f\"Assigning SampleMutID {row['SampleMutID']} (row {index})\")\n",
    "        all_df.loc[index,'index'] = new_index\n",
    "        offset = 1\n",
    "        while hasOverlap(all_df.query(\"index > -1\")):\n",
    "            print(f\"SampleMutID {row['SampleMutID']} hasOverlap in index {new_index}!\")\n",
    "            # move to next index in line using offset\n",
    "            new_index = get_min_index(all_df, offset=offset, black_list=black_list)\n",
    "            all_df.loc[index,'index'] = new_index\n",
    "            offset += 1\n",
    "        print(f\"SampleMutID {row['SampleMutID']} has been assigned to index {new_index}!\")\n",
    "    \n",
    "    # bringing in the non_overlapping bulk mutations\n",
    "    total_df = pd.concat([all_df, df.query('index > -1').drop('internal_overlap', axis=1)]).reset_index(drop=True).sort_values([\"index\", \"Well\"])\n",
    "    return total_df\n",
    "    \n",
    "\n",
    "def add_info(all_df):\n",
    "    '''\n",
    "    custom function to bring certain data from the main df into the index pool output\n",
    "    add stuff to the info field which will be used in the pool output\n",
    "    '''\n",
    "    all_df['info'] = all_df['SampleID'] + \" (\" + all_df['Well'].astype(str) + \")\"\n",
    "    all_df['index'] = all_df['index'].astype(int)\n",
    "    # if no status, then status = \"G\"\n",
    "    all_df.loc[all_df['status'] == \"\", \"status\"] = \"G\"\n",
    "    # add info for non \"G\"\n",
    "    all_df.loc[all_df['status'] != \"G\", 'info'] = all_df['info'] + \" (\" + all_df['status'] + \")\"\n",
    "    # add epic fail for \"F\"\n",
    "    all_df.loc[all_df['status'].str.startswith(\"F\"), 'info'] = \"EPIC FAIL------\" + all_df['info'] + \"---------EPIC FAIL\"\n",
    "    all_df = all_df.sort_values(['index', \"SampleID\", \"Well\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def make_index_table(org_df):\n",
    "    '''\n",
    "    creates a table with a column per index\n",
    "    '''\n",
    "    \n",
    "    df = add_info(org_df)\n",
    "    # get the maximum rows for initiating a clean df\n",
    "    max_rows = get_index_df(df)['sampleCount'].max()\n",
    "\n",
    "    table_df = pd.DataFrame(index=range(max_rows))\n",
    "    for i in df['index'].unique():\n",
    "        table_df[f\"index{i}\"] = df.query('index == @i').reset_index()['info']\n",
    "    return table_df\n",
    "\n",
    "\n",
    "def save2excel(total_df, out_excel):\n",
    "    \n",
    "    with pd.ExcelWriter(out_excel, engine=\"openpyxl\") as writer:\n",
    "        total_df.to_excel(writer, sheet_name=\"all\", index=False)\n",
    "        index_df = get_index_df(total_df)\n",
    "        index_df.to_excel(writer, sheet_name=\"index\", index=False)\n",
    "        table_df = make_index_table(total_df)\n",
    "        table_df.to_excel(writer, sheet_name=\"pools\", index=False)\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = [f\"NHL{i}-{t}\" for i in [1,2,8] for t in [\"WB\", \"GR\"]]\n",
    "\n",
    "total_df = merge_indices(nhl_df, coop_df, black_list=black_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index with the fewest sample for equal allocation\n",
    "def get_min_index(df, black_list=[], white_list=[], offset=0):\n",
    "    '''\n",
    "    get the index containing fewest samples from samples exluding black_list\n",
    "    '''\n",
    "    # remove -1 index\n",
    "    df = df.query(\"index != -1\")\n",
    "    \n",
    "    # exclude samples from black_list\n",
    "    df = df.query(\"SampleID not in @black_list\").query(\"index not in @black_list\")\n",
    "    if len(white_list):\n",
    "        df = df.query(\"SampleID in @white_list or index in @white_list \")\n",
    "    index_df = get_index_df(df).sort_values(\"sampleCount\", ascending=True).iloc[offset]['index']\n",
    "    \n",
    "    return index_df\n",
    "\n",
    "black_list = [f\"NHL{i}-{t}\" for i in [1,2,8] for t in [\"WB\", \"GR\"]]\n",
    "index_df = get_index_df(nhl_df)\n",
    "index_df.sort_values(\"sampleCount\")[:10]\n",
    "get_min_index(nhl_df, offset=1, black_list=black_list, white_list=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    offset = 0\n",
    "    black_list = [f\"NHL{i}-{t}\" for i in [1,2,8] for t in [\"WB\", \"GR\"]]\n",
    "    black_list = []\n",
    "    print(black_list)\n",
    "    white_list = [1,2]\n",
    "    df = nhl_df.copy()\n",
    "    # remove -1 index\n",
    "    df = df.query(\"index != -1\")\n",
    "    \n",
    "    # exclude samples from black_list\n",
    "    df = df.query(\"SampleID not in @black_list\").query(\"index not in @black_list\")\n",
    "    if len(white_list):\n",
    "        print(\"Using white liste\")\n",
    "        df = df.query(\"SampleID in @white_list or index in @white_list \")\n",
    "        \n",
    "    df\n",
    "    index_df = get_index_df(df).sort_values(\"sampleCount\", ascending=True).iloc[offset]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.query('Project == \"AG_Kroenke\"')\n",
    "get_index_df(total_df)\n",
    "\n",
    "# add_info(total_df)\n",
    "# table_df = make_index_table(total_df)\n",
    "# table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ the NHL1-pools (index 0 to 9 all have just 36 samples each)\n",
    "+ cycle through these indizes and divide the coop-samples to these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_excel = os.path.join(folder, \"NebNextSetupTest.xlsx\")\n",
    "save2excel(total_df, out_excel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
